Observação 1: Atividade 1-Exercício 2:a taxa de aprendizado afeta a trajetória e para qual mínimo o gradiente descendente vai.Taxas pequenas garantem estabilidade, mas demoram mais e taxas grandes podem causar salto sobre os mínimos, oscilações ou divergência.

Observação 2: Atividade 2-Exercício 3:se você aumentar o valor de 𝛼 pode acontecer overshooting: os passos passam direto do mínimo e ele nunca converge.Pode começar a oscilar entre os dois lados de um vale.Se for grande demais, o algoritmo pode divergir (sair da curva completamente).Se você diminuir 𝛼 os passos são menores, então o algoritmo leva mais tempo para chegar a um mínimo.Porém, a trajetória se torna mais estável e confiável.

Observação 3: Atividade 2-Exercício 4:taxas muito altas podem causar divergência ou saltos que impedem a convergência.Taxas muito pequenas tornam o aprendizado estável, mas lento.Por causa dos múltiplos mínimos locais, a posição inicial influencia fortemente para onde o algoritmo converge.
