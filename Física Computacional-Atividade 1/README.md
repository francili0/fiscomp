ObservaÃ§Ã£o 1: Atividade 1-ExercÃ­cio 2:a taxa de aprendizado afeta a trajetÃ³ria e para qual mÃ­nimo o gradiente descendente vai.Taxas pequenas garantem estabilidade, mas demoram mais e taxas grandes podem causar salto sobre os mÃ­nimos, oscilaÃ§Ãµes ou divergÃªncia.

ObservaÃ§Ã£o 2: Atividade 2-ExercÃ­cio 3:se vocÃª aumentar o valor de ğ›¼ pode acontecer overshooting: os passos passam direto do mÃ­nimo e ele nunca converge.Pode comeÃ§ar a oscilar entre os dois lados de um vale.Se for grande demais, o algoritmo pode divergir (sair da curva completamente).Se vocÃª diminuir ğ›¼ os passos sÃ£o menores, entÃ£o o algoritmo leva mais tempo para chegar a um mÃ­nimo.PorÃ©m, a trajetÃ³ria se torna mais estÃ¡vel e confiÃ¡vel.

ObservaÃ§Ã£o 3: Atividade 2-ExercÃ­cio 4:taxas muito altas podem causar divergÃªncia ou saltos que impedem a convergÃªncia.Taxas muito pequenas tornam o aprendizado estÃ¡vel, mas lento.Por causa dos mÃºltiplos mÃ­nimos locais, a posiÃ§Ã£o inicial influencia fortemente para onde o algoritmo converge.
